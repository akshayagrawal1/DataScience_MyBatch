{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network for classification\n",
    "784 , 64 , 16 , 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\srikanth\\Anaconda\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Modified National Instuite of Science and technology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = input_data.read_data_sets('./mnist_data',one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mnist\n",
    "# 70,000 images\n",
    "# 55,000 training dataset\n",
    "# 10,000 testing\n",
    "# 5,000 validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training images\n",
    "x_train= mnist.train.images\n",
    "y_train= mnist.train.labels\n",
    "# testing images\n",
    "x_test= mnist.test.images\n",
    "y_test= mnist.test.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(9,6))\n",
    "for i in range(1,7):\n",
    "\n",
    "    plt.subplot(2,3,i)\n",
    "    plt.imshow(x_train[i:i+1].reshape((28,28)),cmap='gray')\n",
    "    plt.title(\"Label: {}\".format(y_train[i:i+1].argmax()))\n",
    "    plt.xticks([]),plt.yticks([])\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### FeedForward Neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('Input_Layer'):\n",
    "    x_inputs = tf.placeholder(dtype=tf.float32,shape=[None,784],name='input')\n",
    "with tf.name_scope('Weights'):\n",
    "    # weights\n",
    "    wx = tf.Variable(tf.truncated_normal(shape=(784,64)))\n",
    "    wy = tf.Variable(tf.truncated_normal(shape=(64,16)))\n",
    "    wz = tf.Variable(tf.truncated_normal(shape=(16,10)))\n",
    "    # bais\n",
    "    bx = tf.Variable(tf.truncated_normal(shape=[64])) \n",
    "    by = tf.Variable(tf.truncated_normal(shape=[16]))\n",
    "    bz = tf.Variable(tf.truncated_normal(shape=[10]))\n",
    "    \n",
    "with tf.name_scope('Layer_1'):\n",
    "    L1 = tf.matmul(x_inputs,wx,name='L1')\n",
    "    L11 = tf.nn.sigmoid(L1+bx,name='Layer1')\n",
    "\n",
    "with tf.name_scope('Layer_2'):\n",
    "    L2 = tf.matmul(L11,wy,name='L2')\n",
    "    L22 = tf.nn.sigmoid(L2+by,name='Layer2')\n",
    "    \n",
    "with tf.name_scope('output'):\n",
    "    L3 = tf.matmul(L22,wz,name='L3')\n",
    "    y = tf.nn.sigmoid(L3+bz,name='output') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### Backward Pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('loss'):\n",
    "    y_actual = tf.placeholder(dtype=tf.float32,shape=[None,10],name='actual')\n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(\n",
    "        labels=y_actual,logits=y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('optimizer'):\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=0.01)\n",
    "    train = optimizer.minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_actual,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(cross_prediction,tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 1001\n",
    "batch_size = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batching(last,batch_size):\n",
    "    return np.random.randint(low=0,high=last,size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 2.31, Acc: 0.11\n",
      "Epoch: 100, Loss: 1.62, Acc: 0.90\n",
      "Epoch: 200, Loss: 1.54, Acc: 0.94\n",
      "Epoch: 300, Loss: 1.52, Acc: 0.94\n",
      "Epoch: 400, Loss: 1.52, Acc: 0.95\n",
      "Epoch: 500, Loss: 1.51, Acc: 0.95\n",
      "Epoch: 600, Loss: 1.51, Acc: 0.96\n",
      "Epoch: 700, Loss: 1.51, Acc: 0.96\n",
      "Epoch: 800, Loss: 1.51, Acc: 0.95\n",
      "Epoch: 900, Loss: 1.50, Acc: 0.96\n",
      "Epoch: 1000, Loss: 1.50, Acc: 0.96\n"
     ]
    }
   ],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    writer = tf.summary.FileWriter('./neural_class',graph=sess.graph)\n",
    "    \n",
    "    # training\n",
    "    for i in range(epochs):\n",
    "        ind = batching(55000,batch_size)\n",
    "        sess.run(train,feed_dict={x_inputs:x_train[ind],\n",
    "                                  y_actual:y_train[ind]})\n",
    "        \n",
    "        # check values for every 100 iterations\n",
    "        if i%100 == 0:\n",
    "            ind1 = batching(10000,batch_size)\n",
    "            loss, acc = sess.run([cost,accuracy],feed_dict={x_inputs:x_test[ind1],\n",
    "                                  y_actual:y_test[ind1]})\n",
    "            \n",
    "            print('Epoch: %d, Loss: %0.2f, Acc: %0.2f'%(i,loss,acc))\n",
    "            \n",
    "    \n",
    "    saver.save(sess,'./numberclass_nn/nn.ckpt')\n",
    "    writer.close()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step - 1 : convert image into gray scale\n",
    "img = cv2.imread('1887.jpg',0) # reading in grayscale\n",
    "# step-2: inverse colors\n",
    "img_not = cv2.bitwise_not(img)\n",
    "# step-3: resize into 28 x 28 \n",
    "img_resize = cv2.resize(img_not,(28,28))\n",
    "# step-4: Normalization 0- 1\n",
    "img_norm = img_resize/img_resize.max()\n",
    "# step -3 : reshape into 1 x 784\n",
    "test = img_norm.reshape((1,784))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 784)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### restore the model and testing it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./numberclass_nn/nn.ckpt\n",
      "[[1.8514884e-03 1.5807255e-04 9.9450374e-01 3.8597926e-01 1.7580111e-05\n",
      "  5.9610064e-04 1.3753629e-04 4.1371284e-04 4.3767787e-04 8.8758306e-06]]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    saver.restore(sess,'./numberclass_nn/nn.ckpt')\n",
    "    result = sess.run(y,feed_dict={x_inputs:test})\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.argmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
